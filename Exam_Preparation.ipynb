{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBZf46fS1rnd"
      },
      "source": [
        "# Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P54UegIRK2i1"
      },
      "source": [
        "#### Full Batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Adam_single_variable_batch(X, y, beta1, beta2, alpha, epsilon, maxNumIters):\n",
        "\n",
        "    gradient_check = 0.001\n",
        "    cost_check = 0.001\n",
        "    v_0 = 0\n",
        "    v_1 = 0\n",
        "    v_0_hat = 0\n",
        "    v_1_hat = 0\n",
        "    m_0 = 0\n",
        "    m_1 = 0\n",
        "    m_0_hat = 0\n",
        "    m_1_hat = 0\n",
        "    theta_0 = 0\n",
        "    theta_1 = 0\n",
        "    \n",
        "    thetas_0_list = []\n",
        "    thetas_1_list = []\n",
        "    cost = []\n",
        "    hypothesis = []\n",
        "    for i in range(maxNumIters):\n",
        "        print(f'****************** Epoch {i} ********************')\n",
        "        print()\n",
        "\n",
        "        m = len(X)\n",
        "\n",
        "        thetas_0_list.append(theta_0)\n",
        "        thetas_1_list.append(theta_1)\n",
        "\n",
        "        # 2- Prediction\n",
        "        h = theta_0 + theta_1 * X\n",
        "        hypothesis.append(h)\n",
        "\n",
        "        # 3- Evaluate Prediction (Calculate loss/cost function)\n",
        "        error = h - y\n",
        "\n",
        "        # j = np.sum(error**2) / (2*m) # --> Normal equation\n",
        "        # j = np.linalg.norm(error)**2 / (2*m) # --> By vector Norm\n",
        "        mse = error @ error / (2*m) # --> By dot product\n",
        "\n",
        "        j = np.sum(mse)\n",
        "        cost.append(j)\n",
        "\n",
        "        # 4- Get gradient\n",
        "        d_theta_0 = np.sum(error) / m\n",
        "        d_theta_1 = np.sum(error*X) / m\n",
        "\n",
        "        d_theta = np.array([[d_theta_0],\n",
        "                            [d_theta_1]])\n",
        "        \n",
        "        d_theta_norm = np.linalg.norm(d_theta)\n",
        "\n",
        "        # 5- Update parameters\n",
        "        m_0 = (beta_1*m_0) + ((1-beta_1) * d_theta_0)\n",
        "        m_1 = (beta_1*m_1) + ((1-beta_1) * d_theta_1)\n",
        "        \n",
        "        m_0_hat = m_0 / (1 - beta_1)\n",
        "        m_1_hat = m_1 / (1 - beta_1)\n",
        "        \n",
        "        v_0 = (beta_2*v_0) + ((1-beta_2) * (d_theta_0**2))\n",
        "        v_1 = (beta_2*v_1) + ((1-beta_2) * (d_theta_1**2))\n",
        "\n",
        "        v_0_hat = v_0 / (1 - beta_2)\n",
        "        v_1_hat = v_1 / (1 - beta_2)\n",
        "\n",
        "        theta_0 = theta_0 - ( alpha / ( np.sqrt(v_0_hat) + epsilon ) * m_0_hat )\n",
        "        theta_1 = theta_1 - ( alpha / ( np.sqrt(v_1_hat) + epsilon ) * m_1_hat )\n",
        "\n",
        "        print('h(x):', h)\n",
        "        print()\n",
        "        \n",
        "        print('Error Vector:\\n', error)\n",
        "        print()\n",
        "        \n",
        "        print('j = ', j)\n",
        "        print()\n",
        "\n",
        "        print('Gradient Vector:\\n', d_theta)\n",
        "        print()\n",
        "        \n",
        "        print('Gradient Vector Norm:\\n', d_theta_norm)\n",
        "        print()\n",
        "\n",
        "        if i > 0:\n",
        "            if d_theta_norm <= gradient_check or abs(cost[-2] - cost[-1]) <= cost_check:\n",
        "                break\n",
        "        \n",
        "        print(f'm_0_{i} : ', m_0)\n",
        "        print(f'm_1_{i} : ', m_1)\n",
        "\n",
        "        print()\n",
        "        print(f'm_0_{i}_corrected : ', m_0_hat)\n",
        "        print(f'm_1_{i}_corrected : ', m_1_hat)\n",
        "        print()\n",
        "\n",
        "        print(f'v_0_{i} : ', v_0)\n",
        "        print(f'v_1_{i} : ', v_1)\n",
        "        print()\n",
        "\n",
        "        print(f'v_0_{i}_corrected : ', v_0_hat)\n",
        "        print(f'v_1_{i}_corrected : ', v_1_hat)\n",
        "\n",
        "\n",
        "    print('****************** Training Report ********************')\n",
        "    print()\n",
        "\n",
        "    print(f'Gradient Descent converged after {i+1} epochs')\n",
        "    print()\n",
        "\n",
        "    print('theta_0_Opt : ', theta_0)\n",
        "    print('theta_1_Opt : ', theta_1)\n",
        "    print()\n",
        "\n",
        "    print('Error Vector:\\n', error)\n",
        "    print()\n",
        "\n",
        "    print('Cost = ', j)\n",
        "    print()\n",
        "\n",
        "    print('h(x) = y_predict:\\n', h)\n",
        "    print()\n",
        "\n",
        "    print('y_actual:\\n', y)\n",
        "\n",
        "    return thetas_0_list, thetas_1_list, cost, hypothesis, thetas_0_list[-1], thetas_1_list[-1]"
      ],
      "metadata": {
        "id": "9p2vbrut3MeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Adam_multi_variable_batch(X, y, beta1, beta2, alpha, epsilon, maxNumIters, ones=False):\n",
        "\n",
        "    # Append ones to first columns if 'ones' parameter is true\n",
        "    if ones:\n",
        "      X = np.insert(X, 0, 1, axis=1)\n",
        "\n",
        "    gradient_check = 0.001\n",
        "    cost_check = 0.001\n",
        "\n",
        "    v = np.zeros((X.shape[1], 1))\n",
        "    v_hat = np.zeros((X.shape[1], 1))\n",
        "    m = np.zeros((X.shape[1], 1))\n",
        "    m_hat = np.zeros((X.shape[1], 1))\n",
        "    theta = np.zeros((X.shape[1], 1))\n",
        "    \n",
        "    theta_list = []\n",
        "    cost = []\n",
        "    cost_per_epoch = []\n",
        "    hypothesis = []\n",
        "    for i in range(maxNumIters):\n",
        "        print(f'****************** Epoch {i} ********************')\n",
        "        print()\n",
        "\n",
        "        theta_list.append(theta)\n",
        "\n",
        "        # 2- Prediction\n",
        "\n",
        "        h = X @ theta\n",
        "        hypothesis.append(h)\n",
        "\n",
        "        # 3- Evaluate Prediction (Calculate loss/cost function)\n",
        "        error = h - y\n",
        "        j = error.T @ error / (2*len(X))\n",
        "        cost.append(j.squeeze())\n",
        "\n",
        "        # 4- Get gradient\n",
        "        d_theta = X.T @ error / len(X)\n",
        "\n",
        "        # 5- Update parameters\n",
        "        m = (beta_1*m) + ((1-beta_1) * d_theta)\n",
        "        \n",
        "        m_hat = m / (1 - beta_1)\n",
        "        \n",
        "        v = (beta_2*v) + ((1-beta_2) * (d_theta**2))\n",
        "\n",
        "        v_hat = v / (1 - beta_2)\n",
        "\n",
        "        theta = theta - ( alpha / ( np.sqrt(v_hat) + epsilon ) * m_hat )\n",
        "\n",
        "        d_theta_norm = np.linalg.norm(d_theta)\n",
        "\n",
        "        print('h(x):', h)\n",
        "        print()\n",
        "        \n",
        "        print('Error Vector:\\n', error)\n",
        "        print()\n",
        "        \n",
        "        print('j = ', j)\n",
        "        print()\n",
        "\n",
        "        print('Gradient Vector:\\n', d_theta)\n",
        "        print()\n",
        "        \n",
        "        print('Gradient Vector Norm:\\n', d_theta_norm)\n",
        "        print()\n",
        "\n",
        "        if i > 0:\n",
        "            if d_theta_norm <= gradient_check or abs(cost[-1] - cost[-2]) <= cost_check:\n",
        "                break\n",
        "        \n",
        "        print(f'm_{i} : ', m)\n",
        "\n",
        "        print()\n",
        "        print(f'm_{i}_corrected : ', m_hat)\n",
        "        print()\n",
        "\n",
        "        print(f'v_{i} : ', v)\n",
        "        print()\n",
        "\n",
        "        print(f'v_{i}_corrected : ', v_hat)\n",
        "\n",
        "\n",
        "    print('****************** Training Report ********************')\n",
        "    print()\n",
        "\n",
        "    print(f'Gradient Descent converged after {i+1} epochs')\n",
        "    print()\n",
        "\n",
        "    print('theta_Opt : ', theta)\n",
        "    print()\n",
        "\n",
        "    print('Error Vector:\\n', error)\n",
        "    print()\n",
        "\n",
        "    print('Cost = ', j)\n",
        "    print()\n",
        "\n",
        "    print('h(x) = y_predict:\\n', h)\n",
        "    print()\n",
        "\n",
        "    print('y_actual:\\n', y)\n",
        "\n",
        "    return theta_list, cost, hypothesis, theta_list[-1]"
      ],
      "metadata": {
        "id": "r_XNlfdRYBun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGfdZ7pkKupb"
      },
      "source": [
        "#### Mini Batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGE00ri11rne"
      },
      "outputs": [],
      "source": [
        "def Adam_single_variable_mini_batch(X, y, beta1, beta2, alpha, epsilon, maxNumIters, batch_size=None):\n",
        "\n",
        "    # if batch not specified, consider it as full batch\n",
        "    if not batch_size:\n",
        "      batch_size = len(X)\n",
        "    \n",
        "    gradient_check = 0.001\n",
        "    cost_check = 0.001\n",
        "    v_0 = 0\n",
        "    v_1 = 0\n",
        "    v_0_hat = 0\n",
        "    v_1_hat = 0\n",
        "    m_0 = 0\n",
        "    m_1 = 0\n",
        "    m_0_hat = 0\n",
        "    m_1_hat = 0\n",
        "    theta_0 = 0\n",
        "    theta_1 = 0\n",
        "    \n",
        "    thetas_0_list = []\n",
        "    thetas_1_list = []\n",
        "    cost = []\n",
        "    cost_per_epoch = []\n",
        "    hypothesis = []\n",
        "    for i in range(maxNumIters):\n",
        "        print(f'****************** Epoch {i} ********************')\n",
        "        print()\n",
        "\n",
        "        for j in range(len(X) // batch_size):\n",
        "\n",
        "            thetas_0_list.append(theta_0)\n",
        "            thetas_1_list.append(theta_1)\n",
        "\n",
        "            # 2- Prediction\n",
        "            start = j * batch_size\n",
        "            end = j * batch_size + batch_size\n",
        "\n",
        "            h = theta_0 + theta_1 * X[start:end]\n",
        "            hypothesis.append(h)\n",
        "\n",
        "            # 3- Evaluate Prediction (Calculate loss/cost function)\n",
        "            error = h - y[start:end]\n",
        "            mse = error @ error / (2*batch_size)\n",
        "            j = np.sum(mse)\n",
        "            cost.append(j)\n",
        "\n",
        "            # 4- Get gradient\n",
        "            d_theta_0 = np.sum(error) / batch_size\n",
        "            d_theta_1 = np.sum(error * X[start:end]) / batch_size\n",
        "\n",
        "            # 5- Update parameters\n",
        "            m_0 = (beta_1*m_0) + ((1-beta_1) * d_theta_0)\n",
        "            m_1 = (beta_1*m_1) + ((1-beta_1) * d_theta_1)\n",
        "            \n",
        "            m_0_hat = m_0 / (1 - beta_1)\n",
        "            m_1_hat = m_1 / (1 - beta_1)\n",
        "            \n",
        "            v_0 = (beta_2*v_0) + ((1-beta_2) * (d_theta_0**2))\n",
        "            v_1 = (beta_2*v_1) + ((1-beta_2) * (d_theta_1**2))\n",
        "\n",
        "            v_0_hat = v_0 / (1 - beta_2)\n",
        "            v_1_hat = v_1 / (1 - beta_2)\n",
        "\n",
        "            theta_0 = theta_0 - ( alpha / ( np.sqrt(v_0_hat) + epsilon ) * m_0_hat )\n",
        "            theta_1 = theta_1 - ( alpha / ( np.sqrt(v_1_hat) + epsilon ) * m_1_hat )\n",
        "\n",
        "        d_theta = np.array([[d_theta_0],\n",
        "                            [d_theta_1]])\n",
        "        \n",
        "        d_theta_norm = np.linalg.norm(d_theta)\n",
        "\n",
        "        cost_per_epoch.append(j)\n",
        "\n",
        "        print('h(x):', h)\n",
        "        print()\n",
        "        \n",
        "        print('Error Vector:\\n', error)\n",
        "        print()\n",
        "        \n",
        "        print('j = ', j)\n",
        "        print()\n",
        "\n",
        "        print('Gradient Vector:\\n', d_theta)\n",
        "        print()\n",
        "        \n",
        "        print('Gradient Vector Norm:\\n', d_theta_norm)\n",
        "        print()\n",
        "\n",
        "        if i > 0:\n",
        "            if d_theta_norm <= gradient_check or abs(cost_per_epoch[-2] - cost_per_epoch[-1]) <= cost_check:\n",
        "                break\n",
        "        \n",
        "        print(f'm_0_{i} : ', m_0)\n",
        "        print(f'm_1_{i} : ', m_1)\n",
        "\n",
        "        print()\n",
        "        print(f'm_0_{i}_corrected : ', m_0_hat)\n",
        "        print(f'm_1_{i}_corrected : ', m_1_hat)\n",
        "        print()\n",
        "\n",
        "        print(f'v_0_{i} : ', v_0)\n",
        "        print(f'v_1_{i} : ', v_1)\n",
        "        print()\n",
        "\n",
        "        print(f'v_0_{i}_corrected : ', v_0_hat)\n",
        "        print(f'v_1_{i}_corrected : ', v_1_hat)\n",
        "\n",
        "\n",
        "    print('****************** Training Report ********************')\n",
        "    print()\n",
        "\n",
        "    print(f'Gradient Descent converged after {i+1} epochs')\n",
        "    print()\n",
        "\n",
        "    print('theta_0_Opt : ', theta_0)\n",
        "    print('theta_1_Opt : ', theta_1)\n",
        "    print()\n",
        "\n",
        "    print('Error Vector:\\n', error)\n",
        "    print()\n",
        "\n",
        "    print('Cost = ', j)\n",
        "    print()\n",
        "\n",
        "    print('h(x) = y_predict:\\n', h)\n",
        "    print()\n",
        "\n",
        "    print('y_actual:\\n', y)\n",
        "\n",
        "    return thetas_0_list, thetas_1_list, cost, hypothesis, thetas_0_list[-1], thetas_1_list[-1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Adam_multi_variable_mini_batch(X, y, beta1, beta2, alpha, epsilon, maxNumIters, batch_size=None, ones=False):\n",
        "\n",
        "    # if batch not specified, consider it as full batch\n",
        "    if not batch_size:\n",
        "      batch_size = len(X)\n",
        "    \n",
        "    # Append ones to first columns if 'ones' parameter is true\n",
        "    if ones:\n",
        "      X = np.insert(X, 0, 1, axis=1)\n",
        "\n",
        "    gradient_check = 0.001\n",
        "    cost_check = 0.001\n",
        "\n",
        "    v = np.zeros((X.shape[1], 1))\n",
        "    v_hat = np.zeros((X.shape[1], 1))\n",
        "    m = np.zeros((X.shape[1], 1))\n",
        "    m_hat = np.zeros((X.shape[1], 1))\n",
        "    theta = np.zeros((X.shape[1], 1))\n",
        "    \n",
        "    theta_list = []\n",
        "    cost = []\n",
        "    cost_per_epoch = []\n",
        "    hypothesis = []\n",
        "    for i in range(maxNumIters):\n",
        "        print(f'****************** Epoch {i} ********************')\n",
        "        print()\n",
        "\n",
        "        for j in range(len(X) // batch_size):\n",
        "\n",
        "            theta_list.append(theta)\n",
        "\n",
        "            # 2- Prediction\n",
        "            start = j * batch_size\n",
        "            end = j * batch_size + batch_size\n",
        "\n",
        "            h = X[start:end] @ theta\n",
        "            hypothesis.append(h)\n",
        "\n",
        "            # 3- Evaluate Prediction (Calculate loss/cost function)\n",
        "            error = h - y[start:end]\n",
        "            j = error.T @ error / (2*batch_size)\n",
        "            cost.append(j.squeeze())\n",
        "\n",
        "            # 4- Get gradient\n",
        "            d_theta = X[start:end].T @ error / batch_size\n",
        "\n",
        "            # 5- Update parameters\n",
        "            m = (beta_1*m) + ((1-beta_1) * d_theta)\n",
        "            \n",
        "            m_hat = m / (1 - beta_1)\n",
        "            \n",
        "            v = (beta_2*v) + ((1-beta_2) * (d_theta**2))\n",
        "\n",
        "            v_hat = v / (1 - beta_2)\n",
        "\n",
        "            theta = theta - ( alpha / ( np.sqrt(v_hat) + epsilon ) * m_hat )\n",
        "\n",
        "\n",
        "        d_theta_norm = np.linalg.norm(d_theta)\n",
        "\n",
        "        cost_per_epoch.append(j)\n",
        "\n",
        "        print('h(x):', h)\n",
        "        print()\n",
        "        \n",
        "        print('Error Vector:\\n', error)\n",
        "        print()\n",
        "        \n",
        "        print('j = ', j)\n",
        "        print()\n",
        "\n",
        "        print('Gradient Vector:\\n', d_theta)\n",
        "        print()\n",
        "        \n",
        "        print('Gradient Vector Norm:\\n', d_theta_norm)\n",
        "        print()\n",
        "\n",
        "        if i > 0:\n",
        "            if d_theta_norm <= gradient_check or abs(cost_per_epoch[-1] - cost_per_epoch[-2]) <= cost_check:\n",
        "                break\n",
        "        \n",
        "        print(f'm_{i} : ', m)\n",
        "\n",
        "        print()\n",
        "        print(f'm_{i}_corrected : ', m_hat)\n",
        "        print()\n",
        "\n",
        "        print(f'v_{i} : ', v)\n",
        "        print()\n",
        "\n",
        "        print(f'v_{i}_corrected : ', v_hat)\n",
        "\n",
        "\n",
        "    print('****************** Training Report ********************')\n",
        "    print()\n",
        "\n",
        "    print(f'Gradient Descent converged after {i+1} epochs')\n",
        "    print()\n",
        "\n",
        "    print('theta_Opt : ', theta)\n",
        "    print()\n",
        "\n",
        "    print('Error Vector:\\n', error)\n",
        "    print()\n",
        "\n",
        "    print('Cost = ', j)\n",
        "    print()\n",
        "\n",
        "    print('h(x) = y_predict:\\n', h)\n",
        "    print()\n",
        "\n",
        "    print('y_actual:\\n', y)\n",
        "\n",
        "    return theta_list, cost, hypothesis, theta_list[-1]"
      ],
      "metadata": {
        "id": "eCCT2ViwYEHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI6ahYiVK1iI"
      },
      "source": [
        "#### Stochastic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Adam_single_variable_stochastic(X, y, beta1, beta2, alpha, epsilon, maxNumIters):\n",
        "\n",
        "    gradient_check = 0.001\n",
        "    cost_check = 0.001\n",
        "    v_0 = 0\n",
        "    v_1 = 0\n",
        "    v_0_hat = 0\n",
        "    v_1_hat = 0\n",
        "    m_0 = 0\n",
        "    m_1 = 0\n",
        "    m_0_hat = 0\n",
        "    m_1_hat = 0\n",
        "    theta_0 = 0\n",
        "    theta_1 = 0\n",
        "    \n",
        "    thetas_0_list = []\n",
        "    thetas_1_list = []\n",
        "    cost = []\n",
        "    cost_per_epoch = []\n",
        "    hypothesis = []\n",
        "    for epoch in range(maxNumIters):\n",
        "        print(f'****************** Epoch {epoch} ********************')\n",
        "        print()\n",
        "\n",
        "        for i in range(len(X)):\n",
        "\n",
        "            thetas_0_list.append(theta_0)\n",
        "            thetas_1_list.append(theta_1)\n",
        "\n",
        "            # 2- Prediction\n",
        "\n",
        "            h = theta_0 + theta_1 * X[i]\n",
        "            hypothesis.append(h)\n",
        "\n",
        "            # 3- Evaluate Prediction (Calculate loss/cost function)\n",
        "            error = h - y[i]\n",
        "            j = error**2 / 2\n",
        "            cost.append(j)\n",
        "\n",
        "            # 4- Get gradient\n",
        "            d_theta_0 = error\n",
        "            d_theta_1 = error * X[i]\n",
        "\n",
        "            # 5- Update parameters\n",
        "            m_0 = (beta_1*m_0) + ((1-beta_1) * d_theta_0)\n",
        "            m_1 = (beta_1*m_1) + ((1-beta_1) * d_theta_1)\n",
        "            \n",
        "            m_0_hat = m_0 / (1 - beta_1)\n",
        "            m_1_hat = m_1 / (1 - beta_1)\n",
        "            \n",
        "            v_0 = (beta_2*v_0) + ((1-beta_2) * (d_theta_0**2))\n",
        "            v_1 = (beta_2*v_1) + ((1-beta_2) * (d_theta_1**2))\n",
        "\n",
        "            v_0_hat = v_0 / (1 - beta_2)\n",
        "            v_1_hat = v_1 / (1 - beta_2)\n",
        "\n",
        "            theta_0 = theta_0 - ( alpha / ( np.sqrt(v_0_hat) + epsilon ) * m_0_hat )\n",
        "            theta_1 = theta_1 - ( alpha / ( np.sqrt(v_1_hat) + epsilon ) * m_1_hat )\n",
        "\n",
        "        d_theta = np.array([[d_theta_0],\n",
        "                            [d_theta_1]])\n",
        "        \n",
        "        d_theta_norm = np.linalg.norm(d_theta)\n",
        "\n",
        "        cost_per_epoch.append(j)\n",
        "\n",
        "        print('h(x):', h)\n",
        "        print()\n",
        "        \n",
        "        print('Error Vector:\\n', error)\n",
        "        print()\n",
        "        \n",
        "        print('j = ', j)\n",
        "        print()\n",
        "\n",
        "        print('Gradient Vector:\\n', d_theta)\n",
        "        print()\n",
        "        \n",
        "        print('Gradient Vector Norm:\\n', d_theta_norm)\n",
        "        print()\n",
        "\n",
        "        if epoch > 0:\n",
        "            if d_theta_norm <= gradient_check or abs(cost_per_epoch[-2] - cost_per_epoch[-1]) <= cost_check:\n",
        "                break\n",
        "        \n",
        "        print(f'm_0_{epoch} : ', m_0)\n",
        "        print(f'm_1_{epoch} : ', m_1)\n",
        "\n",
        "        print()\n",
        "        print(f'm_0_{epoch}_corrected : ', m_0_hat)\n",
        "        print(f'm_1_{epoch}_corrected : ', m_1_hat)\n",
        "        print()\n",
        "\n",
        "        print(f'v_0_{epoch} : ', v_0)\n",
        "        print(f'v_1_{epoch} : ', v_1)\n",
        "        print()\n",
        "\n",
        "        print(f'v_0_{epoch}_corrected : ', v_0_hat)\n",
        "        print(f'v_1_{epoch}_corrected : ', v_1_hat)\n",
        "\n",
        "\n",
        "    print('****************** Training Report ********************')\n",
        "    print()\n",
        "\n",
        "    print(f'Gradient Descent converged after {epoch+1} epochs')\n",
        "    print()\n",
        "\n",
        "    print('theta_0_Opt : ', theta_0)\n",
        "    print('theta_1_Opt : ', theta_1)\n",
        "    print()\n",
        "\n",
        "    print('Error Vector:\\n', error)\n",
        "    print()\n",
        "\n",
        "    print('Cost = ', j)\n",
        "    print()\n",
        "\n",
        "    print('h(x) = y_predict:\\n', h)\n",
        "    print()\n",
        "\n",
        "    print('y_actual:\\n', y)\n",
        "\n",
        "    return thetas_0_list, thetas_1_list, cost, hypothesis, thetas_0_list[-1], thetas_1_list[-1]"
      ],
      "metadata": {
        "id": "9ZMhM8Zb5xTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Adam_multi_variable_stochastic(X, y, beta1, beta2, alpha, epsilon, maxNumIters, ones=False):\n",
        "    \n",
        "    # Append ones to first columns if 'ones' parameter is true\n",
        "    if ones:\n",
        "      X = np.insert(X, 0, 1, axis=1)\n",
        "\n",
        "    gradient_check = 0.001\n",
        "    cost_check = 0.001\n",
        "\n",
        "    v = np.zeros((X.shape[1], 1))\n",
        "    v_hat = np.zeros((X.shape[1], 1))\n",
        "    m = np.zeros((X.shape[1], 1))\n",
        "    m_hat = np.zeros((X.shape[1], 1))\n",
        "    theta = np.zeros((X.shape[1], 1))\n",
        "    \n",
        "    theta_list = []\n",
        "    cost = []\n",
        "    cost_per_epoch = []\n",
        "    hypothesis = []\n",
        "    for epoch in range(maxNumIters):\n",
        "        print(f'****************** Epoch {epoch} ********************')\n",
        "        print()\n",
        "\n",
        "        for i in range(len(X)):\n",
        "\n",
        "            theta_list.append(theta)\n",
        "\n",
        "            # 2- Prediction\n",
        "\n",
        "            h = X[i] @ theta\n",
        "            hypothesis.append(h)\n",
        "\n",
        "            # 3- Evaluate Prediction (Calculate loss/cost function)\n",
        "            error = h - y[i]\n",
        "            j = error.T @ error / 2\n",
        "            cost.append(j.squeeze())\n",
        "\n",
        "            # 4- Get gradient\n",
        "            d_theta = X[i].T * error\n",
        "\n",
        "            # 5- Update parameters\n",
        "            m = (beta_1*m) + ((1-beta_1) * d_theta)\n",
        "            \n",
        "            m_hat = m / (1 - beta_1)\n",
        "            \n",
        "            v = (beta_2*v) + ((1-beta_2) * (d_theta**2))\n",
        "\n",
        "            v_hat = v / (1 - beta_2)\n",
        "\n",
        "            theta = theta - ( alpha / ( np.sqrt(v_hat) + epsilon ) * m_hat )\n",
        "\n",
        "\n",
        "        d_theta_norm = np.linalg.norm(d_theta)\n",
        "\n",
        "        cost_per_epoch.append(j)\n",
        "\n",
        "        print('h(x):', h)\n",
        "        print()\n",
        "        \n",
        "        print('Error Vector:\\n', error)\n",
        "        print()\n",
        "        \n",
        "        print('j = ', j)\n",
        "        print()\n",
        "\n",
        "        print('Gradient Vector:\\n', d_theta)\n",
        "        print()\n",
        "        \n",
        "        print('Gradient Vector Norm:\\n', d_theta_norm)\n",
        "        print()\n",
        "\n",
        "        if epoch > 0:\n",
        "          if d_theta_norm <= gradient_check or abs(cost_per_epoch[-1] - cost_per_epoch[-2]) <= cost_check:\n",
        "                break\n",
        "        \n",
        "        print(f'm_{epoch} : ', m)\n",
        "\n",
        "        print()\n",
        "        print(f'm_{epoch}_corrected : ', m_hat)\n",
        "        print()\n",
        "\n",
        "        print(f'v_{epoch} : ', v)\n",
        "        print()\n",
        "\n",
        "        print(f'v_{epoch}_corrected : ', v_hat)\n",
        "\n",
        "\n",
        "    print('****************** Training Report ********************')\n",
        "    print()\n",
        "\n",
        "    print(f'Gradient Descent converged after {epoch+1} epochs')\n",
        "    print()\n",
        "\n",
        "    print('theta_Opt : ', theta)\n",
        "    print()\n",
        "\n",
        "    print('Error Vector:\\n', error)\n",
        "    print()\n",
        "\n",
        "    print('Cost = ', j)\n",
        "    print()\n",
        "\n",
        "    print('h(x) = y_predict:\\n', h)\n",
        "    print()\n",
        "\n",
        "    print('y_actual:\\n', y)\n",
        "\n",
        "    return theta_list, cost, hypothesis, theta_list[-1]"
      ],
      "metadata": {
        "id": "dv2jWT2XYG9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4MZCv1zq5OF"
      },
      "source": [
        "# BFGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuDH562iKoos"
      },
      "source": [
        "#### Full Batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def BFGS_multi_variable_batch(X, y, maxNumIters, alpha=1, ones=False):\n",
        "\n",
        "  if ones:\n",
        "    X = np.insert(X, 0, 1, axis=1)\n",
        "\n",
        "  # 1- Initialize parameters and hyper-paramters\n",
        "  theta_cur = np.array(X.shape[1] * [[0.01]])\n",
        "  theta_prev = np.zeros((X.shape[1], 1))\n",
        "\n",
        "  I = np.eye(X.shape[1])\n",
        "  B_inv = np.eye(X.shape[1])\n",
        "  \n",
        "  loss_threshold = 1e-6\n",
        "\n",
        "  theta_list = []\n",
        "  cost = []\n",
        "  hypothesis = []\n",
        "  for i in range(maxNumIters):\n",
        "      print(f'****************** Iteration {i} ********************')\n",
        "      print()\n",
        "\n",
        "      theta_list.append(theta_cur)\n",
        "\n",
        "\n",
        "      m = len(X)\n",
        "\n",
        "      # 2- Prediction\n",
        "      h_cur = X @ theta_cur\n",
        "      h_prev = X @ theta_prev\n",
        "      hypothesis.append(h_cur)\n",
        "\n",
        "\n",
        "      # 3- Evaluate Prediction (Calculate loss/cost function)\n",
        "      error_cur = h_cur - y\n",
        "      error_prev = h_prev - y\n",
        "\n",
        "      # j = np.sum(error_cur**2) / (2*m) # --> Normal equation\n",
        "      # j = np.linalg.norm(error_cur)**2 / (2*m) # --> By vector Norm\n",
        "      j = error_cur.T @ error_cur / (2*m) # --> By dot product\n",
        "\n",
        "      cost.append(j.squeeze())\n",
        "\n",
        "      # 4- Get gradient\n",
        "      d_theta_cur = X.T @ error_cur / m\n",
        "      d_theta_prev = X.T @ error_prev / m\n",
        "      \n",
        "      d_theta_cur_norm = np.linalg.norm(d_theta_cur)\n",
        "\n",
        "      # 5- Update parameters\n",
        "      delta_theta = theta_cur - theta_prev\n",
        "      delta_y = d_theta_cur - d_theta_prev\n",
        "      B_inv = (I - ((delta_theta @ delta_y.T) / (delta_y.T @ delta_theta))) @ B_inv @ (I -\n",
        "                                                                    ((delta_y @ delta_theta.T) / (delta_y.T @ delta_theta))) + ((delta_theta @ delta_theta.T) / (delta_y.T @ delta_theta))\n",
        "      theta_prev = theta_cur\n",
        "      theta_cur = theta_cur - alpha * (B_inv @ d_theta_cur)\n",
        "\n",
        "      print('h(x):\\n', h_cur)\n",
        "      print()\n",
        "      \n",
        "      print('Error Vector:\\n', error_cur)\n",
        "      print()\n",
        "      \n",
        "      print('j = ', j)\n",
        "      print()\n",
        "\n",
        "      print('Gradient Vector:\\n', d_theta_cur)\n",
        "      print()\n",
        "      \n",
        "      print('Gradient Vector Norm:\\n', d_theta_cur_norm)\n",
        "      print()\n",
        "\n",
        "      if abs(np.linalg.norm(d_theta_cur)) <= loss_threshold:\n",
        "          break\n",
        "      \n",
        "      print('thetas_new : \\n ', theta_cur)\n",
        "      print()\n",
        "\n",
        "\n",
        "  print('****************** Training Report ********************')\n",
        "  print()\n",
        "\n",
        "  print(f'Gradient Descent converged after {i} iterations')\n",
        "  print()\n",
        "\n",
        "  print('theta_Opt : \\n', theta_cur)\n",
        "  print()\n",
        "\n",
        "  print('Error Vector:\\n', error_cur)\n",
        "  print()\n",
        "\n",
        "  print('Cost = ', j)\n",
        "  print()\n",
        "\n",
        "  print('h(x) = y_predict:\\n', h_cur)\n",
        "  print()\n",
        "\n",
        "  print('y_actual:\\n', y)\n",
        "\n",
        "  return theta_list, cost, hypothesis, theta_list[-1]\n",
        "\n"
      ],
      "metadata": {
        "id": "vsr-dOe3nf9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aR4Dgu4Ks-u"
      },
      "source": [
        "#### Mini Batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def BFGS_multi_variable_mini_batch(X, y, maxNumIters, alpha=1, batch_size=None, ones=False):\n",
        "\n",
        "  # if batch not specified, consider it as full batch\n",
        "  if not batch_size:\n",
        "    batch_size = len(X)\n",
        "\n",
        "  # Append ones to first columns if 'ones' parameter is true\n",
        "  if ones:\n",
        "    X = np.insert(X, 0, 1, axis=1)\n",
        "\n",
        "  # 1- Initialize parameters and hyper-paramters\n",
        "  theta_cur = np.array(X.shape[1] * [[0.01]])\n",
        "  theta_prev = np.zeros((X.shape[1], 1))\n",
        "\n",
        "  I = np.eye(X.shape[1])\n",
        "  B_inv = np.eye(X.shape[1])\n",
        "  \n",
        "  loss_threshold = 1e-6\n",
        "\n",
        "  theta_list = []\n",
        "  cost = []\n",
        "  hypothesis = []\n",
        "  for epoch in range(maxNumIters):\n",
        "      print(f'****************** Iteration {epoch} ********************')\n",
        "      print()\n",
        "\n",
        "      for i in range(len(X) // batch_size):\n",
        "\n",
        "        theta_list.append(theta_cur)\n",
        "\n",
        "        # 2- Prediction\n",
        "        start = i * batch_size\n",
        "        end = i * batch_size + batch_size\n",
        "\n",
        "        h_cur = X[start:end] @ theta_cur\n",
        "        h_prev = X[start:end] @ theta_prev\n",
        "        hypothesis.append(h_cur)\n",
        "\n",
        "\n",
        "        # 3- Evaluate Prediction (Calculate loss/cost function)\n",
        "        error_cur = h_cur - y[start:end]\n",
        "        error_prev = h_prev - y[start:end]\n",
        "\n",
        "        # j = np.sum(error_cur**2) / (2*m) # --> Normal equation\n",
        "        # j = np.linalg.norm(error_cur)**2 / (2*m) # --> By vector Norm\n",
        "        j = error_cur.T @ error_cur / (2*batch_size) # --> By dot product\n",
        "\n",
        "        cost.append(j.squeeze())\n",
        "\n",
        "        # 4- Get gradient\n",
        "        d_theta_cur = X[start:end].T @ error_cur / batch_size\n",
        "        d_theta_prev = X[start:end].T @ error_prev / batch_size\n",
        "        \n",
        "        d_theta_cur_norm = np.linalg.norm(d_theta_cur)\n",
        "\n",
        "        # 5- Update parameters\n",
        "        delta_theta = theta_cur - theta_prev\n",
        "        delta_y = d_theta_cur - d_theta_prev\n",
        "        B_inv = (I - ((delta_theta @ delta_y.T) / (delta_y.T @ delta_theta))) @ B_inv @ (I -\n",
        "                                                                      ((delta_y @ delta_theta.T) / (delta_y.T @ delta_theta))) + ((delta_theta @ delta_theta.T) / (delta_y.T @ delta_theta))\n",
        "        theta_prev = theta_cur\n",
        "        theta_cur = theta_cur - alpha * (B_inv @ d_theta_cur)\n",
        "\n",
        "      print('h(x):\\n', h_cur)\n",
        "      print()\n",
        "      \n",
        "      print('Error Vector:\\n', error_cur)\n",
        "      print()\n",
        "      \n",
        "      print('j = ', j)\n",
        "      print()\n",
        "\n",
        "      print('Gradient Vector:\\n', d_theta_cur)\n",
        "      print()\n",
        "      \n",
        "      print('Gradient Vector Norm:\\n', d_theta_cur_norm)\n",
        "      print()\n",
        "\n",
        "      if abs(np.linalg.norm(d_theta_cur)) <= loss_threshold:\n",
        "          break\n",
        "      \n",
        "      print('thetas_new : \\n ', theta_cur)\n",
        "      print()\n",
        "\n",
        "\n",
        "  print('****************** Training Report ********************')\n",
        "  print()\n",
        "\n",
        "  print(f'Gradient Descent converged after {epoch} iterations')\n",
        "  print()\n",
        "\n",
        "  print('theta_Opt : \\n', theta_cur)\n",
        "  print()\n",
        "\n",
        "  print('Error Vector:\\n', error_cur)\n",
        "  print()\n",
        "\n",
        "  print('Cost = ', j)\n",
        "  print()\n",
        "\n",
        "  print('h(x) = y_predict:\\n', h_cur)\n",
        "  print()\n",
        "\n",
        "  print('y_actual:\\n', y)\n",
        "\n",
        "  return theta_list, cost, hypothesis, theta_list[-1]\n",
        "\n"
      ],
      "metadata": {
        "id": "fqkuVuwdF0PD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}